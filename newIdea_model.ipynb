{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6554f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Raffaele\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Raffaele\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import syft as sy\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08e28a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset_M = MNIST(root='./data', train=True, download=True, transform=transform_mnist)\n",
    "test_dataset_M = MNIST(root='./data', train=False, download=True, transform=transform_mnist)\n",
    "\n",
    "\n",
    "# Extract features (images) and labels\n",
    "#x_train = train_dataset.data.float()  # Convert images to float tensor\n",
    "#y_train = train_dataset.targets       # Extract labels\n",
    "\n",
    "#x_test = test_dataset.data.float()    # Convert images to float tensor\n",
    "#y_test = test_dataset.targets          # Extract labels\n",
    "\n",
    "train_subset = Subset(train_dataset_M, range(10000))\n",
    "test_subset = Subset(test_dataset_M, range(1000))\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader_M = DataLoader(dataset=train_subset, batch_size=batch_size, shuffle=True)\n",
    "test_loader_M = DataLoader(dataset=test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Define transformations\n",
    "transform_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cifar)\n",
    "trainsubset = Subset(trainset, range(10000))\n",
    "trainloader_C = torch.utils.data.DataLoader(trainsubset, batch_size=10, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cifar)\n",
    "testsubset = Subset(testset, range(1000))\n",
    "testloader_C = torch.utils.data.DataLoader(testsubset, batch_size=10, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4782af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (images) and labels from the train set\n",
    "#x_train_cifar = trainset.data.transpose((0, 3, 1, 2)).astype(np.float32) / 255.0  # Convert images to float tensor and normalize\n",
    "#y_train_cifar = trainset.targets  # Extract labels\n",
    "\n",
    "# Extract features (images) and labels from the test set\n",
    "#x_test_cifar = testset.data.transpose((0, 3, 1, 2)).astype(np.float32) / 255.0  # Convert images to float tensor and normalize\n",
    "#y_test_cifar = testset.targets  # Extract labels     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788a6532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 32, 32])\n",
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in trainloader_C:\n",
    "    print(images.shape)\n",
    "    break\n",
    "for images, labels in train_loader_M:\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31eba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client1(nn.Module):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        super(Client1, self).__init__()\n",
    "\n",
    "        self.num_samples = x_train.shape[0]\n",
    "        self.x_train = x_train\n",
    "        self.labels = y_train\n",
    "        self.batch = 10\n",
    "        \n",
    "        \n",
    "\n",
    "        self.encoder_model = nn.Sequential(nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1,1), padding='valid'),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1), padding='valid'),\n",
    "                                    nn.ReLU()\n",
    "                                    )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        self.latent_model = nn.Sequential(\n",
    "                                nn.MaxPool2d(kernel_size=(2,2)),\n",
    "                                nn.Flatten()\n",
    "                                 )\n",
    "\n",
    "    \n",
    "        self.decoder_model = nn.Sequential(nn.Linear(64*12*12, 10),\n",
    "                                     nn.Softmax(dim=1))\n",
    "        \n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=0.001)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs_encoder = self.encoder_model(inputs)\n",
    "        outputs_latent = self.latent_model(outputs_encoder)\n",
    "        latent_parameters = list(self.latent_model.parameters())\n",
    "        outputs_class = self.decoder_model(outputs_latent)\n",
    "        return outputs_class, latent_parameters\n",
    "\n",
    "    \n",
    "    def update(self, latent_parameters):\n",
    "        # Get the parameters of the latent model\n",
    "        latent_params = self.latent_model.parameters()\n",
    "    \n",
    "        # Iterate over the parameters and update them with new values\n",
    "        for param, new_value in zip(latent_params, latent_parameters):\n",
    "            param.data.copy_(new_value)\n",
    "    \n",
    "    \n",
    "    def train_step(self, inputs, labels):\n",
    "        \n",
    "        #Forward pass\n",
    "        logits, latent_parameters = self.forward(inputs)\n",
    "\n",
    "        #Compute the loss\n",
    "        loss = self.loss_function(logits, labels)\n",
    "\n",
    "        #Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #updare the parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), latent_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28be489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client2(nn.Module):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        super(Client2, self).__init__()\n",
    "\n",
    "        self.num_samples = x_train.shape[0]\n",
    "        self.x_train = x_train\n",
    "        self.labels = y_train\n",
    "        self.batch = 10\n",
    "        \n",
    "        \n",
    "\n",
    "        self.encoder_model = nn.Sequential(\n",
    "                                    nn.Conv2d(3, 1, kernel_size=5, stride=1),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                                    nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1,1), padding='valid'),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1), padding='valid'),\n",
    "                                    nn.ReLU()\n",
    "                                    )\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        self.latent_model = nn.Sequential(\n",
    "                                nn.MaxPool2d(kernel_size=(2,2)),\n",
    "                                nn.Flatten()\n",
    "                                 )\n",
    "\n",
    "    \n",
    "        self.decoder_model = nn.Sequential(nn.Linear(1600, 10),\n",
    "                                     nn.Softmax(dim=1))\n",
    "        \n",
    "        self.optimizer = optim.SGD(self.parameters(), lr=0.001)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs_encoder = self.encoder_model(inputs)\n",
    "        outputs_latent = self.latent_model(outputs_encoder)\n",
    "        latent_parameters = list(self.latent_model.parameters())\n",
    "        outputs_class = self.decoder_model(outputs_latent)\n",
    "        return outputs_class, latent_parameters\n",
    "\n",
    "\n",
    "    def update(self, latent_parameters):\n",
    "        # Get the parameters of the latent model\n",
    "        latent_params = self.latent_model.parameters()\n",
    "    \n",
    "        # Iterate over the parameters and update them with new values\n",
    "        for param, new_value in zip(latent_params, latent_parameters):\n",
    "            param.data.copy_(new_value)\n",
    "    \n",
    "    def train_step(self, inputs, labels):\n",
    "        \n",
    "        #Forward pass\n",
    "        logits, latent_parameters = self.forward(inputs)\n",
    "\n",
    "        #Compute the loss\n",
    "        loss = self.loss_function(logits, labels)\n",
    "\n",
    "        #Zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        #updare the parameters\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item(), latent_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56cca1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for images, labels in train_loader_M:\n",
    "    x_train.append(images)\n",
    "    y_train.append(labels)\n",
    "    \n",
    "for images_test, labels_test in test_loader_M:\n",
    "    x_test.append(images_test)\n",
    "    y_test.append(labels_test)\n",
    "\n",
    "# Concatenate the batches to obtain the full datasets\n",
    "x_train = torch.cat(x_train, dim=0)\n",
    "y_train = torch.cat(y_train, dim=0)\n",
    "\n",
    "x_test = torch.cat(x_test, dim=0)\n",
    "y_test = torch.cat(y_test, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c3a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_C = []\n",
    "y_train_C = []\n",
    "x_test_C = []\n",
    "y_test_C = []\n",
    "\n",
    "for images, labels in trainloader_C:\n",
    "    x_train_C.append(images)\n",
    "    y_train_C.append(labels)\n",
    "    \n",
    "for images_test, labels_test in testloader_C:\n",
    "    x_test_C.append(images_test)\n",
    "    y_test_C.append(labels_test)\n",
    "\n",
    "# Concatenate the batches to obtain the full datasets\n",
    "x_train_C = torch.cat(x_train_C, dim=0)\n",
    "y_train_C = torch.cat(y_train_C, dim=0)\n",
    "\n",
    "x_test_C = torch.cat(x_test_C, dim=0)\n",
    "y_test_C = torch.cat(y_test_C, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2412a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.2739\n",
      "Epoch [1/10], Loss: 2.3041\n",
      "Epoch [2/10], Loss: 2.0908\n",
      "Epoch [2/10], Loss: 2.3022\n",
      "2\n",
      "Epoch [3/10], Loss: 2.0885\n",
      "Epoch [3/10], Loss: 2.3026\n",
      "Epoch [4/10], Loss: 1.6269\n",
      "Epoch [4/10], Loss: 2.3010\n",
      "4\n",
      "Epoch [5/10], Loss: 1.8855\n",
      "Epoch [5/10], Loss: 2.3024\n",
      "Epoch [6/10], Loss: 1.7509\n",
      "Epoch [6/10], Loss: 2.3030\n",
      "6\n",
      "Epoch [7/10], Loss: 1.6108\n",
      "Epoch [7/10], Loss: 2.3032\n",
      "Epoch [8/10], Loss: 1.7979\n",
      "Epoch [8/10], Loss: 2.3016\n",
      "8\n",
      "Epoch [9/10], Loss: 1.9284\n",
      "Epoch [9/10], Loss: 2.3010\n",
      "Epoch [10/10], Loss: 1.6208\n",
      "Epoch [10/10], Loss: 2.3024\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "#Instatiate your Client1 model\n",
    "client1 = Client1(x_train, y_train)\n",
    "client2 = Client2(x_train_C, y_train_C)\n",
    "\n",
    "num_epochs = 10\n",
    "latent_parameters = []\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    client1.train()\n",
    "    client2.train()\n",
    "    \n",
    "    for i, ((images_M, labels_M), (images_C, labels_C)) in enumerate(zip(train_loader_M, trainloader_C)):\n",
    "        loss_1, latent_parameters_1 = client1.train_step(images_M, labels_M)\n",
    "        loss_2, latent_parameters_2 = client2.train_step(images_C, labels_C)\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss_1:.4f}\")\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss_2:.4f}\")\n",
    "    if ((epoch+1)%2 == 0):\n",
    "        print(epoch+1)\n",
    "        latent_parameters =[(x + y) / 2 for x, y in zip(latent_parameters_1, latent_parameters_2)]\n",
    "        client1.update(latent_parameters)\n",
    "        client2.update(latent_parameters)\n",
    "    \n",
    "    latent_parameters = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81975039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6759999990463257, 0.13500000536441803)\n"
     ]
    }
   ],
   "source": [
    "def test(inputs_1, labels_1, inputs_2, labels_2):\n",
    "        logits_1, _ = client1.forward(inputs_1)\n",
    "        logits_2, _ = client2.forward(inputs_2)\n",
    "        \n",
    "\n",
    "        _, predictions_1 = torch.max(logits_1, 1)\n",
    "        _, predictions_2 = torch.max(logits_2, 1)\n",
    "        \n",
    "        acc_1 = torch.mean((predictions_1 == labels_1).float())\n",
    "        acc_2 = torch.mean((predictions_2 == labels_2).float())\n",
    "\n",
    "        return acc_1.item(), acc_2.item() \n",
    "print(test(x_test, y_test, x_test_C, y_test_C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
